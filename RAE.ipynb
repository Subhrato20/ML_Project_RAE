{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv7TYnczGN7Q",
        "outputId": "d47f7b08-95e2-4b9b-fbe4-b339357e3b5f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive._mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhzLS4puGqei"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Log setting\n",
        "logging.basicConfig(format=\"%(asctime)s %(levelname)s %(message)s\", datefmt=\"%H:%M:%S\", level=logging.INFO)\n",
        "\n",
        "# Change display.max_rows to show all features.\n",
        "pd.set_option(\"display.max_rows\", 85)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zAgdkTfGx0U"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bUH0OGbGz1X"
      },
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/train_MachineLearningCVE.csv', skipinitialspace=True)\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/test_MachineLearningCVE.csv', skipinitialspace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5-UpRwZJfFM"
      },
      "source": [
        "df = pd.concat([df_train, df_test], axis=0, copy=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC_Yt4vYJj15"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def preprocessing(df: pd.DataFrame) -> (np.ndarray, np.ndarray):\n",
        "    # Shuffle the dataset\n",
        "    df = df.sample(frac=1)\n",
        "\n",
        "    # Split features and labels\n",
        "    x = df.iloc[:, df.columns != 'Label']\n",
        "    y = df[['Label']].to_numpy()\n",
        "\n",
        "    # Scale the features between 0 ~ 1\n",
        "    scaler = MinMaxScaler()\n",
        "    x = scaler.fit_transform(x)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "x, y = preprocessing(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5F9PMkmJoKx"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(x, y, train_size=0.7, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vdqeS-FJ6Lt"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "import itertools\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score,f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.layers import Input,Dropout,Dense\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "from keras.utils.data_utils import get_file\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRwY0aefO-RF"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils import data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbkfGD4aPD7M"
      },
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(49, 500),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(500, 250),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(250, 50),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(50, 3),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(3, 50),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(50, 250),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(250, 500),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(500, 49),\n",
        "            #nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f5sYIcgPMO6"
      },
      "source": [
        "\n",
        "def L(X, X_, t):\n",
        "    if t == 'mse':\n",
        "        l=nn.MSELoss()\n",
        "        return l(X, X_)\n",
        "     \n",
        "\n",
        "def R(X):\n",
        "    return torch.mm(X, torch.t(X))\n",
        "\n",
        "def tau(X, t):\n",
        "    return torch.where(X < t, X.float(), torch.zeros(X.shape).float())\n",
        "\n",
        "def rae_loss(alpha, t, L_type='mse'):\n",
        "    def rae(y_true, y_pred):\n",
        "        return (1 - alpha)*L(y_true, y_pred, L_type) + alpha*L(tau(R(y_true), t), tau(R(y_pred), t), L_type)\n",
        "    return rae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUGOZWyrPVEv",
        "outputId": "5459dc35-fe66-4f10-9801-3f149665ca86"
      },
      "source": [
        "alphas = np.linspace(0, 1, 15)\n",
        "t = 1\n",
        "alpha=alphas[1]\n",
        "#alpha=0.05\n",
        "#rae_loss(alpha, t, 'mse')\n",
        "print(alpha)\n",
        "print(alphas)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.07142857142857142\n",
            "[0.         0.07142857 0.14285714 0.21428571 0.28571429 0.35714286\n",
            " 0.42857143 0.5        0.57142857 0.64285714 0.71428571 0.78571429\n",
            " 0.85714286 0.92857143 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN604B81PaRW"
      },
      "source": [
        "def model_training(autoencoder, train_loader, epoch):\n",
        "    #loss_metric = nn.MSELoss()\n",
        "    loss_metric =rae_loss(alpha, t, 'mse')\n",
        "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    \n",
        "\n",
        "    autoencoder.train()\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        DataPoint = data.float()\n",
        "        DataPoint = Variable(DataPoint)\n",
        "        DataPoint = DataPoint.view(DataPoint.size(0), -1)\n",
        "        #if cuda: DataPoint = DataPoint.to(device)\n",
        "        outputs = autoencoder(DataPoint)\n",
        "        loss = loss_metric(outputs, DataPoint)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (i + 1) % LOG_INTERVAL == 0:\n",
        "            print('Epoch [{}/{}] - Iter[{}/{}], HUBER loss:{:.4f}'.format(\n",
        "                epoch + 1, EPOCHS, i + 1, len(X_train) // BATCH_SIZE, loss.item()\n",
        "                ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i67a5XfuP0Fp"
      },
      "source": [
        "def evaluation(autoencoder, test_loader):\n",
        "    total_loss = 0\n",
        "    #loss_metric = nn.MSELoss()\n",
        "    loss_metric =rae_loss(alpha, t, 'mse')\n",
        "    autoencoder.eval()\n",
        "    for i, data in enumerate(test_loader):\n",
        "        DataPoint = data.float()\n",
        "        DataPoint = Variable(DataPoint)\n",
        "        DataPoint = DataPoint.view(DataPoint.size(0), -1)\n",
        "        #if cuda: DataPoint = DataPoint.to(device)\n",
        "        outputs = autoencoder(DataPoint)\n",
        "        loss = loss_metric(outputs, DataPoint)\n",
        "        total_loss += loss * len(DataPoint)\n",
        "    avg_loss = total_loss / len(test_loader.dataset)\n",
        "\n",
        "    print('\\nAverage MSE Loss on Test set: {:.4f}'.format(avg_loss))\n",
        "\n",
        "    global BEST_VAL\n",
        "    if TRAIN_SCRATCH and avg_loss < BEST_VAL:\n",
        "        BEST_VAL = avg_loss\n",
        "        torch.save(autoencoder.state_dict(), './simple_autoencoder.pt')\n",
        "        print('Save Best Model\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "hPhglQRUP4lc",
        "outputId": "88933f2f-bb39-4c1e-ff6e-fcf497905ad5"
      },
      "source": [
        "import datetime\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    EPOCHS = 100\n",
        "    BATCH_SIZE = 100\n",
        "    LEARNING_RATE = 1e-3\n",
        "    WEIGHT_DECAY = 1e-5\n",
        "    LOG_INTERVAL = 100\n",
        "    TRAIN_SCRATCH = True        # whether to train a model from scratch\n",
        "    BEST_VAL = float('inf')     # record the best val loss\n",
        "\n",
        "    #train_loader, test_loader = data_utils.load_mnist(BATCH_SIZE)\n",
        "    torch.manual_seed(39)\n",
        "\n",
        "    autoencoder = Autoencoder()\n",
        "    #if cuda: autoencoder.to(device)\n",
        "\n",
        "    if TRAIN_SCRATCH:\n",
        "        # Training autoencoder from scratch\n",
        "        for epoch in range(EPOCHS):\n",
        "            starttime = datetime.datetime.now()\n",
        "            model_training(autoencoder, df_train, epoch)\n",
        "            endtime = datetime.datetime.now()\n",
        "            print(f'Train a epoch in {(endtime - starttime).seconds} seconds')\n",
        "            # evaluate on test set and save best model\n",
        "            evaluation(autoencoder, test_loader)\n",
        "        print('Trainig Complete with best validation loss {:.4f}'.format(BEST_VAL))\n",
        "\n",
        "    else:\n",
        "        autoencoder.load_state_dict(torch.load('/simple_autoencoder.pt'))\n",
        "        evaluation(autoencoder, test_loader)\n",
        "\n",
        "        autoencoder.cpu()\n",
        "        dataiter = iter(X_train)\n",
        "        DataPoint, _ = next(dataiter)\n",
        "        DataPoint = Variable(DataPoint[:32])\n",
        "        outputs = autoencoder(DataPoint.view(DataPoint.size(0), -1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-81e2b820299f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mstarttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mmodel_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Train a epoch in {(endtime - starttime).seconds} seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-d361e5a7439d>\u001b[0m in \u001b[0;36mmodel_training\u001b[0;34m(autoencoder, train_loader, epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mDataPoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mDataPoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataPoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mDataPoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataPoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataPoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'float'"
          ]
        }
      ]
    }
  ]
}